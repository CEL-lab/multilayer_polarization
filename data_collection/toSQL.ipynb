{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paketler vs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sqlite database\n",
    "conn = sqlite3.connect('twitter004.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### github to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x28d173a35c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sqlite tweets table using columns from above\n",
    "cursor.execute('''CREATE TABLE githubTweets(\n",
    "              id_p INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "              author_id TEXT,\n",
    "              text TEXT,\n",
    "              context_annotations JSON,\n",
    "              conversation_id TEXT,\n",
    "              created_at TEXT,\n",
    "              edit_controls TEXT,\n",
    "              entities JSON,\n",
    "              geo TEXT,\n",
    "              id TEXT,\n",
    "              in_reply_to_user_id TEXT,\n",
    "              lang TEXT,\n",
    "              public_metrics JSON,\n",
    "              possibly_sensitive TEXT,\n",
    "              referenced_tweets TEXT,\n",
    "              reply_settings TEXT,\n",
    "              source TEXT,\n",
    "              withheld TEXT,\n",
    "              attachments TEXT,\n",
    "              errors TEXT\n",
    "              )''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['github2.csv', 'github3.csv', 'github4.csv', 'github5.csv', 'githubLast.csv']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get file names to a list in a folder\n",
    "import os\n",
    "fileList = os.listdir(\"./github\")\n",
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github2.csv is done\n",
      "github3.csv is done\n",
      "github4.csv is done\n",
      "github5.csv is done\n",
      "githubLast.csv is done\n"
     ]
    }
   ],
   "source": [
    "for i in fileList:\n",
    "  fileName = \"./github/\" + i\n",
    "  df = pd.read_csv(fileName,  dtype = {'id': str, 'author_id': str, 'conversation_id': str, 'in_reply_to_user_id': str})\n",
    "  df['entities'] = df['entities'].str.replace(\"'\", '\"')\n",
    "  df['public_metrics'] = df['public_metrics'].str.replace(\"'\", '\"')\n",
    "  df['context_annotations'] = df['context_annotations'].str.replace(\"'\", '\"')\n",
    "  df['edit_controls'] = df['edit_controls'].str.replace(\"'\", '\"')\n",
    "  df['attachments'] = df['attachments'].str.replace(\"'\", '\"')\n",
    "  df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "  df.to_sql('githubTweets', conn, if_exists='append', index = False)\n",
    "  print(i + \" is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"legislators-current.csv\",  dtype = {'twitter_id': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql('githubList', conn, if_exists='append', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x28d173a35c0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sqlite tweets table using columns from above\n",
    "cursor.execute('''CREATE TABLE allPPATweets(\n",
    "              id_p INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "              author_id TEXT,\n",
    "              text TEXT,\n",
    "              context_annotations JSON,\n",
    "              conversation_id TEXT,\n",
    "              created_at TEXT,\n",
    "              edit_controls TEXT,\n",
    "              entities JSON,\n",
    "              geo TEXT,\n",
    "              id TEXT,\n",
    "              in_reply_to_user_id TEXT,\n",
    "              lang TEXT,\n",
    "              public_metrics JSON,\n",
    "              possibly_sensitive TEXT,\n",
    "              referenced_tweets TEXT,\n",
    "              reply_settings TEXT,\n",
    "              source TEXT,\n",
    "              withheld TEXT,\n",
    "              attachments TEXT,\n",
    "              errors TEXT\n",
    "              )''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweetsTable301.csv',\n",
       " 'tweetsTable302.csv',\n",
       " 'tweetsTable303.csv',\n",
       " 'tweetsTable304.csv',\n",
       " 'tweetsTable305.csv',\n",
       " 'tweetsTable306.csv',\n",
       " 'tweetsTable307.csv',\n",
       " 'tweetsTable308.csv',\n",
       " 'tweetsTable309.csv',\n",
       " 'tweetsTable310.csv',\n",
       " 'tweetsTable311.csv',\n",
       " 'tweetsTable312.csv',\n",
       " 'tweetsTable313.csv',\n",
       " 'tweetsTable314.csv',\n",
       " 'tweetsTable315.csv',\n",
       " 'tweetsTable316.csv',\n",
       " 'tweetsTable317.csv',\n",
       " 'tweetsTable318.csv',\n",
       " 'tweetsTable319.csv',\n",
       " 'tweetsTable320.csv',\n",
       " 'tweetsTable321.csv',\n",
       " 'tweetsTable322.csv',\n",
       " 'tweetsTable323.csv',\n",
       " 'tweetsTable324.csv',\n",
       " 'tweetsTable325.csv',\n",
       " 'tweetsTable326.csv',\n",
       " 'tweetsTable327.csv',\n",
       " 'tweetsTable328.csv',\n",
       " 'tweetsTable329.csv',\n",
       " 'tweetsTable330.csv',\n",
       " 'tweetsTable331.csv',\n",
       " 'tweetsTable332.csv',\n",
       " 'tweetsTable333.csv',\n",
       " 'tweetsTable334.csv',\n",
       " 'tweetsTable335.csv',\n",
       " 'tweetsTable336.csv',\n",
       " 'tweetsTable337.csv',\n",
       " 'tweetsTable338.csv',\n",
       " 'tweetsTable339.csv',\n",
       " 'tweetsTable340.csv',\n",
       " 'tweetsTable341.csv',\n",
       " 'tweetsTable342.csv',\n",
       " 'tweetsTable343.csv',\n",
       " 'tweetsTable344.csv',\n",
       " 'tweetsTable345.csv',\n",
       " 'tweetsTable346.csv',\n",
       " 'tweetsTable347.csv',\n",
       " 'tweetsTable348.csv',\n",
       " 'tweetsTable349.csv',\n",
       " 'tweetsTable350.csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get file names to a list in a folder\n",
    "import os\n",
    "fileList = os.listdir(\"./tumPPA\")\n",
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetsTable301.csv\n",
      "tweetsTable302.csv\n",
      "tweetsTable303.csv\n",
      "tweetsTable304.csv\n",
      "tweetsTable305.csv\n",
      "tweetsTable306.csv\n",
      "tweetsTable307.csv\n",
      "tweetsTable308.csv\n",
      "tweetsTable309.csv\n",
      "tweetsTable310.csv\n",
      "tweetsTable311.csv\n",
      "tweetsTable312.csv\n",
      "tweetsTable313.csv\n",
      "tweetsTable314.csv\n",
      "tweetsTable315.csv\n",
      "tweetsTable316.csv\n",
      "tweetsTable317.csv\n",
      "tweetsTable318.csv\n",
      "tweetsTable319.csv\n",
      "tweetsTable320.csv\n",
      "tweetsTable321.csv\n",
      "tweetsTable322.csv\n",
      "tweetsTable323.csv\n",
      "tweetsTable324.csv\n",
      "tweetsTable325.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysf\\AppData\\Local\\Temp\\ipykernel_12224\\2111061793.py:3: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fileName,  dtype = {'id': str, 'author_id': str, 'conversation_id': str, 'in_reply_to_user_id': str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetsTable326.csv\n",
      "tweetsTable327.csv\n",
      "tweetsTable328.csv\n",
      "tweetsTable329.csv\n",
      "tweetsTable330.csv\n",
      "tweetsTable331.csv\n",
      "tweetsTable332.csv\n",
      "tweetsTable333.csv\n",
      "tweetsTable334.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysf\\AppData\\Local\\Temp\\ipykernel_12224\\2111061793.py:3: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fileName,  dtype = {'id': str, 'author_id': str, 'conversation_id': str, 'in_reply_to_user_id': str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetsTable335.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysf\\AppData\\Local\\Temp\\ipykernel_12224\\2111061793.py:3: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fileName,  dtype = {'id': str, 'author_id': str, 'conversation_id': str, 'in_reply_to_user_id': str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetsTable336.csv\n",
      "tweetsTable337.csv\n",
      "tweetsTable338.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysf\\AppData\\Local\\Temp\\ipykernel_12224\\2111061793.py:3: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fileName,  dtype = {'id': str, 'author_id': str, 'conversation_id': str, 'in_reply_to_user_id': str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetsTable339.csv\n",
      "tweetsTable340.csv\n",
      "tweetsTable341.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysf\\AppData\\Local\\Temp\\ipykernel_12224\\2111061793.py:3: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fileName,  dtype = {'id': str, 'author_id': str, 'conversation_id': str, 'in_reply_to_user_id': str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetsTable342.csv\n",
      "tweetsTable343.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysf\\AppData\\Local\\Temp\\ipykernel_12224\\2111061793.py:3: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fileName,  dtype = {'id': str, 'author_id': str, 'conversation_id': str, 'in_reply_to_user_id': str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetsTable344.csv\n",
      "tweetsTable345.csv\n",
      "tweetsTable346.csv\n",
      "tweetsTable347.csv\n",
      "tweetsTable348.csv\n",
      "tweetsTable349.csv\n",
      "tweetsTable350.csv\n"
     ]
    }
   ],
   "source": [
    "for i in fileList:\n",
    "  fileName = \"./tumPPA/\" + i\n",
    "  df = pd.read_csv(fileName,  dtype = {'id': str, 'author_id': str, 'conversation_id': str, 'in_reply_to_user_id': str})\n",
    "  df['entities'] = df['entities'].str.replace(\"'\", '\"')\n",
    "  df['public_metrics'] = df['public_metrics'].str.replace(\"'\", '\"')\n",
    "  df['context_annotations'] = df['context_annotations'].str.replace(\"'\", '\"')\n",
    "  df['edit_controls'] = df['edit_controls'].str.replace(\"'\", '\"')\n",
    "  df['attachments'] = df['attachments'].str.replace(\"'\", '\"')\n",
    "  df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "  print(i)\n",
    "  df.to_sql('allPPATweets', conn, if_exists='append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPA = pd.read_excel('../data/PP_USA_01.31.23.xlsx', dtype={'BIOID': str, 'GOVTWIT_ID': str, 'PTWIT_ID': str, 'OTWIT_ID': str, 'OTWIT_ID_2': str, 'OTWIT_ID_3': str, 'OTWIT_ID_4': str, 'OTWIT_ID_5': str, 'OTWIT_ID_6': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12719"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPA.to_sql('allPPAList', conn, if_exists='append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
